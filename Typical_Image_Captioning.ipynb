{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBjDVltwZ7JFeHHOFyvCj8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "82f1fa1cdb014b2b905469d58ecec27e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1eef0818ab1d4c34ad31cd156ab05bb5",
              "IPY_MODEL_10e2780f4eec40349207c20540e1b7f9",
              "IPY_MODEL_78dcaaca8d4d4c53bcfe6fa2c5bba92d"
            ],
            "layout": "IPY_MODEL_73ba7df56f584abb835c545c59f8c9c9"
          }
        },
        "1eef0818ab1d4c34ad31cd156ab05bb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17246e4045e640e592f703c11bbbfc33",
            "placeholder": "​",
            "style": "IPY_MODEL_e4d12e58480a4ef6a67a359593aa355a",
            "value": "100%"
          }
        },
        "10e2780f4eec40349207c20540e1b7f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29365a51f8ba450eae6e2dea21a5e988",
            "max": 178793939,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_34658c04b2fe434bbb5dfad737cf29a7",
            "value": 178793939
          }
        },
        "78dcaaca8d4d4c53bcfe6fa2c5bba92d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9bc839a3046a4ccf92e7696d21f43636",
            "placeholder": "​",
            "style": "IPY_MODEL_e55eb13440c445d19297aae213275cb4",
            "value": " 171M/171M [00:03&lt;00:00, 172MB/s]"
          }
        },
        "73ba7df56f584abb835c545c59f8c9c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17246e4045e640e592f703c11bbbfc33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4d12e58480a4ef6a67a359593aa355a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29365a51f8ba450eae6e2dea21a5e988": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34658c04b2fe434bbb5dfad737cf29a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9bc839a3046a4ccf92e7696d21f43636": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e55eb13440c445d19297aae213275cb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVq0xu1W4tZk",
        "outputId": "04cdc5ec-3c7f-4d51-e89e-75e79e0663d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'zero-shot-image-captioning'...\n",
            "remote: Enumerating objects: 164, done.\u001b[K\n",
            "remote: Counting objects: 100% (75/75), done.\u001b[K\n",
            "remote: Compressing objects: 100% (75/75), done.\u001b[K\n",
            "remote: Total 164 (delta 34), reused 0 (delta 0), pack-reused 89\u001b[K\n",
            "Receiving objects: 100% (164/164), 76.89 MiB | 16.87 MiB/s, done.\n",
            "Resolving deltas: 100% (73/73), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/CornerSiow/zero-shot-image-captioning.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"zero-shot-image-captioning/code/Vocabulary.py\" \"Vocabulary.py\"\n",
        "!cp \"zero-shot-image-captioning/code/DecoderLSTM.py\" \"DecoderLSTM.py\""
      ],
      "metadata": {
        "id": "B3Bv1JhD42u_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from skimage import io\n",
        "from Vocabulary import Vocabulary\n",
        "from DecoderLSTM import DecoderLSTM\n",
        "import random\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "random.seed(10)\n",
        "torch.manual_seed(10)\n",
        "np.random.seed(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElaGAs1T46YS",
        "outputId": "f9c3bee4-5699-4ad1-b0ea-2b33df9257b0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "3KkZEXip6h_6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Dataset"
      ],
      "metadata": {
        "id": "-7_o1jGI8CrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = Vocabulary()\n",
        "vocab.loadFile(\"zero-shot-image-captioning/data/vocab.pickle\")"
      ],
      "metadata": {
        "id": "J_l8Fu_149ke"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainData = [\n",
        "    (\"zero-shot-image-captioning/img_test/working_1.jpg\",\"A person using a laptop in the office\"),\n",
        "    (\"zero-shot-image-captioning/img_test/eating_1.jpg\",\"A person eats a banana in front of a laptop\"),\n",
        "    (\"zero-shot-image-captioning/img_test/washing_1.jpg\",\"A person washes his face in the sink\"),\n",
        "    (\"zero-shot-image-captioning/img_test/cycling_1.jpg\",\"A person riding a bike on a clear sky\"),\n",
        "    (\"zero-shot-image-captioning/img_test/bus_1.jpg\",\"Someone is waiting at the bus stop\")\n",
        "]\n",
        "testData = [\n",
        "    (\"zero-shot-image-captioning/img_test/working_2.jpg\",\"A person using a laptop in the office\"),\n",
        "    (\"zero-shot-image-captioning/img_test/eating_2.jpg\",\"A person eats a banana in front of a laptop\"),\n",
        "    (\"zero-shot-image-captioning/img_test/washing_2.jpg\",\"A person washes his face in the sink\"),\n",
        "    (\"zero-shot-image-captioning/img_test/cycling_2.jpg\",\"A person riding a bike on a clear sky\"),\n",
        "    (\"zero-shot-image-captioning/img_test/bus_2.jpg\",\"Someone is waiting at the bus stop\")\n",
        "]\n",
        "class ImageCaptionDataset(Dataset):\n",
        "    def __init__(self,imagesList, vocab):\n",
        "        self.vocab = vocab\n",
        "        self.imagesList = imagesList\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToTensor(), \n",
        "            transforms.Resize(512), \n",
        "            transforms.CenterCrop(512),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "        self.samples = []\n",
        "        self.captions = []\n",
        "        for v in imagesList:\n",
        "          image = io.imread(v[0])\n",
        "          sample = self.transform(image)\n",
        "          self.samples.append(sample)\n",
        "          self.captions.append(vocab.convertSentenceToToken(v[1]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imagesList)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        sample = self.samples[idx]\n",
        "        caption = self.captions[idx]\n",
        "\n",
        "        return sample,caption\n"
      ],
      "metadata": {
        "id": "AmuM-9795d7h"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(data):\n",
        "    x = []\n",
        "    y = []\n",
        "    for _x, _y in data:\n",
        "        x.append(_x)\n",
        "        y.append(_y)\n",
        "        \n",
        "    y = torch.nn.utils.rnn.pad_sequence(y, batch_first=True)\n",
        "    return torch.stack(x), y\n",
        "\n",
        "trainDataset = ImageCaptionDataset(trainData, vocab)\n",
        "testDataset = ImageCaptionDataset(testData, vocab)\n",
        "trainLoader = DataLoader(trainDataset, batch_size = 1, shuffle = True, collate_fn =collate_fn)\n",
        "testLoader = DataLoader(testDataset, batch_size = 1, shuffle = True, collate_fn =collate_fn)\n"
      ],
      "metadata": {
        "id": "letNPogv62UK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Initialize Encoder and Decoder"
      ],
      "metadata": {
        "id": "a5czrInz8NP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, encoded_image_size=14):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.enc_image_size = encoded_image_size\n",
        "\n",
        "        resnet = torchvision.models.resnet101(pretrained=True)  # pretrained ImageNet ResNet-101\n",
        "\n",
        "        # Remove linear for the feature extraction.\n",
        "        modules = list(resnet.children())[:-1]\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "\n",
        "    def forward(self, images):\n",
        "        out = self.resnet(images)  # (batch_size, 2048, 1, 1)\n",
        "        return out.flatten(1)"
      ],
      "metadata": {
        "id": "-hM00VRJ5vGu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(vocab)\n",
        "embed_size = 1*1*2048\n",
        "hidden_size = 256\n",
        "\n",
        "encoder = Encoder()\n",
        "encoder.to(device)\n",
        "decoder = DecoderLSTM(embed_size, hidden_size, vocab_size)\n",
        "decoder.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248,
          "referenced_widgets": [
            "82f1fa1cdb014b2b905469d58ecec27e",
            "1eef0818ab1d4c34ad31cd156ab05bb5",
            "10e2780f4eec40349207c20540e1b7f9",
            "78dcaaca8d4d4c53bcfe6fa2c5bba92d",
            "73ba7df56f584abb835c545c59f8c9c9",
            "17246e4045e640e592f703c11bbbfc33",
            "e4d12e58480a4ef6a67a359593aa355a",
            "29365a51f8ba450eae6e2dea21a5e988",
            "34658c04b2fe434bbb5dfad737cf29a7",
            "9bc839a3046a4ccf92e7696d21f43636",
            "e55eb13440c445d19297aae213275cb4"
          ]
        },
        "id": "GVsMNGes6ZYp",
        "outputId": "a4aabd2e-d654-4e3a-b611-a829c08a8221"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/171M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "82f1fa1cdb014b2b905469d58ecec27e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecoderLSTM(\n",
              "  (embedding): Embedding(30, 2048)\n",
              "  (lstm): LSTM(2048, 256, bias=False, batch_first=True)\n",
              "  (linear): Linear(in_features=256, out_features=30, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params = list(encoder.parameters()) + list(decoder.parameters())\n",
        "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
        "criterion.to(device)\n",
        "optimizer = torch.optim.Adam(params, lr=0.001, betas=(0.9,0.999), eps=1e-8)"
      ],
      "metadata": {
        "id": "a93klEpi6wpq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start Training"
      ],
      "metadata": {
        "id": "kNg6qLWh8ZBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.train()\n",
        "decoder.train()\n",
        "bar = tqdm(range(1000))\n",
        "for epoch in bar:\n",
        "    totalLoss = 0\n",
        "    for x, y in trainLoader:       \n",
        "        encoder.zero_grad()\n",
        "        decoder.zero_grad()\n",
        "        features = encoder(x.to(device))\n",
        "        \n",
        "  \n",
        "        outputs = decoder(features, y.to(device))\n",
        "        \n",
        "        loss = criterion(outputs.view(-1, vocab_size), y.view(-1).to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        totalLoss += loss.item()\n",
        "    bar.set_description(\"Total Loss: {:.4f}\".format(totalLoss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46DjbQ0v7CrS",
        "outputId": "e5499384-5cd8-4a6d-a6fc-09e154a0540a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Total Loss: 0.2628: 100%|██████████| 1000/1000 [09:45<00:00,  1.71it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start Testing"
      ],
      "metadata": {
        "id": "gvtGV9LQE_yK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.eval()\n",
        "decoder.eval()\n",
        "with torch.no_grad():\n",
        "  for x, y in trainLoader:   \n",
        "    features = encoder(x.to(device))\n",
        "    output = decoder.sample(features.unsqueeze(0)) \n",
        "    sentence = vocab.clean_sentence(output) \n",
        "    ground_truth = vocab.clean_sentence(y.numpy()[0]) \n",
        "    print(sentence, ground_truth)  \n",
        "  for x, y in testLoader:   \n",
        "    features = encoder(x.to(device))\n",
        "    output = decoder.sample(features.unsqueeze(0)) \n",
        "    sentence = vocab.clean_sentence(output) \n",
        "    ground_truth = vocab.clean_sentence(y.numpy()[0]) \n",
        "    print(sentence, ground_truth)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIR75px27_6v",
        "outputId": "da7b0ef4-d863-43c8-e914-bcf3eb6fd6c3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " a person washes his face in the sink  a person washes his face in the sink\n",
            " a person eats a banana in front of a laptop  a person eats a banana in front of a laptop\n",
            " a person eats a banana in front of a laptop  a person using a laptop in the office\n",
            " a person washes his face in the sink  someone is waiting at the bus stop\n",
            " someone is waiting at the bus stop  a person riding a bike on a clear sky\n",
            " someone is waiting at the bus stop  someone is waiting at the bus stop\n",
            " someone is waiting at the bus stop  a person eats a banana in front of a laptop\n",
            " a person washes his face in the sink  a person washes his face in the sink\n",
            " a person washes his face in the sink  a person riding a bike on a clear sky\n",
            " a person washes his face in the sink  a person using a laptop in the office\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "with torch.no_grad():\n",
        "  for file in os.listdir(\"zero-shot-image-captioning/img_test\"):\n",
        "      if file.endswith(\".jpg\"):\n",
        "        img = \"zero-shot-image-captioning/img_test/\" + file\n",
        "        image = io.imread(img)\n",
        "        x = testDataset.transform(image).unsqueeze(0)\n",
        "        features = encoder(x.to(device))\n",
        "        output = decoder.sample(features.unsqueeze(0)) \n",
        "        sentence = vocab.clean_sentence(output) \n",
        "        print(file + \"\\t\"+sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ll9imt58t_o",
        "outputId": "190e79d4-d0a2-47ee-e63b-f75bb8728c46"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bus_2.jpg\t someone is waiting at the bus stop\n",
            "working_2.jpg\t a person washes his face in the sink\n",
            "washing_1.jpg\t a person washes his face in the sink\n",
            "eating_2.jpg\t someone is waiting at the bus stop\n",
            "cycling_2.jpg\t a person washes his face in the sink\n",
            "cycling_1.jpg\t someone is waiting at the bus stop\n",
            "working_1.jpg\t a person eats a banana in front of a laptop\n",
            "bus_1.jpg\t a person washes his face in the sink\n",
            "eating_1.jpg\t a person eats a banana in front of a laptop\n",
            "washing_2.jpg\t a person washes his face in the sink\n"
          ]
        }
      ]
    }
  ]
}