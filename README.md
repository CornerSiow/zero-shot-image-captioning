# Zero-Shot Image Captioning
This is the code and dataset used in this paper.

The reason why have this project is we found out it's hard to have a dataset to caption elderly life (include images), therefore, instead of collecting images from elderly life, we proposed using a model to learn from text, and then used any third-party detection tools to provide the detected symbolic result for the model to generate caption.

Follow the Step 1 to 3 can generate the caption from the images.

Then for evaluation metrics, please run Image_Captioning_Evaluation.ipynb to generate the score.

Moreover, we also compared the proposed model with typical image captioning model: Using encoder encode image.
Please find the file Typical_Image_Captioning.ipynb for more details.

Feel free to use the code and cite the paper.
