{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNsB+CHs0zYHXZ3kWdRoiIO"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Generate The Similar Dictionary Map"
      ],
      "metadata": {
        "id": "aYVx2U--H3rH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the Glove from https://github.com/stanfordnlp/GloVe"
      ],
      "metadata": {
        "id": "Ihe6LTALJMMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# obtain the similar words library from https://github.com/stanfordnlp/GloVe\n",
        "!wget https://nlp.stanford.edu/data/wordvecs/glove.6B.zip\n",
        "!unzip -p glove.6B.zip glove.6B.300d.txt > glove.6B.300d.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFzlM-uCxgUR",
        "outputId": "360eba30-11aa-4308-85eb-f0824b9415a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-10 05:07:45--  https://nlp.stanford.edu/data/wordvecs/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/wordvecs/glove.6B.zip [following]\n",
            "--2022-10-10 05:07:45--  https://downloads.cs.stanford.edu/nlp/data/wordvecs/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182753 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.1’\n",
            "\n",
            "glove.6B.zip.1      100%[===================>] 822.24M  5.00MB/s    in 2m 39s  \n",
            "\n",
            "2022-10-10 05:10:24 (5.18 MB/s) - ‘glove.6B.zip.1’ saved [862182753/862182753]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start Generate the similar words dictionary map"
      ],
      "metadata": {
        "id": "BsywyuS1JS3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "import pickle\n",
        "\n",
        "def generateSimilarDict():\n",
        "  print(\"Reading similar dictionary\")\n",
        "  with open(\"glove.6B.300d.txt\", 'r') as f:\n",
        "    words = [x.rstrip().split(' ')[0] for x in f.readlines()]\n",
        "\n",
        "  print(\"Reading similar dictionary Vectors\")\n",
        "  with open(\"glove.6B.300d.txt\", 'r') as f:\n",
        "    vectors = {}\n",
        "    for line in f:\n",
        "        vals = line.rstrip().split(' ')\n",
        "        vectors[vals[0]] = [float(x) for x in vals[1:]]\n",
        "\n",
        "  vocab_size = len(words)\n",
        "  vocab = {w: idx for idx, w in enumerate(words)}\n",
        "  ivocab = {idx: w for idx, w in enumerate(words)}\n",
        "\n",
        "  vector_dim = len(vectors[ivocab[0]])\n",
        "  W = np.zeros((vocab_size, vector_dim))\n",
        "  for word, v in vectors.items():\n",
        "      if word == '<unk>':\n",
        "          continue\n",
        "      W[vocab[word], :] = v\n",
        "\n",
        "  # normalize each word vector to unit variance\n",
        "  W_norm = np.zeros(W.shape)\n",
        "  d = (np.sum(W ** 2, 1) ** (0.5))\n",
        "  W_norm = (W.T / d).T\n",
        "\n",
        "  print(\"Save Dictionary\")\n",
        "  with open('similar_dict_glove.6B.300d.pickle', 'wb') as handle:\n",
        "    pickle.dump((W_norm, vocab, ivocab), handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "generateSimilarDict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlkVd5YKzuaa",
        "outputId": "3a631b59-1ee7-499b-ae9d-4c8ec12cae08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading similar dictionary\n",
            "Reading similar dictionary Vectors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in true_divide\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Save Dictionary\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 2: Generate Symbolic List"
      ],
      "metadata": {
        "id": "O15HY24mJBFk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4MOs1o6sOc3",
        "outputId": "b7640cc5-07d3-4dfd-ea9a-6569efc4263c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/facebookresearch/detectron2.git\n",
            "  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-ebr_c3p0\n",
            "  Running command git clone -q https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-ebr_c3p0\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (7.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (3.2.2)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (2.0.5)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (2.0.1)\n",
            "Requirement already satisfied: yacs>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (0.1.8)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (0.8.10)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (1.5.0)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (4.64.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (2.8.0)\n",
            "Requirement already satisfied: fvcore<0.1.6,>=0.1.5 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (0.1.5.post20220512)\n",
            "Requirement already satisfied: iopath<0.1.10,>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (0.1.9)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (0.16.0)\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (1.3.0)\n",
            "Requirement already satisfied: omegaconf>=2.1 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (2.2.3)\n",
            "Requirement already satisfied: hydra-core>=1.1 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (1.2.0)\n",
            "Requirement already satisfied: black==22.3.0 in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (22.3.0)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (0.6.11)\n",
            "Requirement already satisfied: fairscale in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (0.4.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from detectron2==0.6) (21.3)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from black==22.3.0->detectron2==0.6) (0.10.1)\n",
            "Requirement already satisfied: typed-ast>=1.4.2 in /usr/local/lib/python3.7/dist-packages (from black==22.3.0->detectron2==0.6) (1.5.4)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from black==22.3.0->detectron2==0.6) (2.0.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.7/dist-packages (from black==22.3.0->detectron2==0.6) (0.4.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.7/dist-packages (from black==22.3.0->detectron2==0.6) (8.1.3)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.7/dist-packages (from black==22.3.0->detectron2==0.6) (4.1.1)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.7/dist-packages (from black==22.3.0->detectron2==0.6) (2.5.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from click>=8.0.0->black==22.3.0->detectron2==0.6) (5.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (1.21.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core>=1.1->detectron2==0.6) (5.9.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.7/dist-packages (from hydra-core>=1.1->detectron2==0.6) (4.9.3)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from iopath<0.1.10,>=0.1.7->detectron2==0.6) (2.5.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.6) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.6) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.6) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->detectron2==0.6) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->detectron2==0.6) (1.15.0)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.7/dist-packages (from fairscale->detectron2==0.6) (1.12.1+cu113)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->click>=8.0.0->black==22.3.0->detectron2==0.6) (3.8.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (1.49.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (0.37.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (1.8.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (57.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (2.23.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (1.2.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (3.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->detectron2==0.6) (1.0.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2==0.6) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2==0.6) (3.2.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from timm->detectron2==0.6) (0.10.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm->detectron2==0.6) (0.13.1+cu113)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->timm->detectron2==0.6) (3.8.0)\n",
            "--2022-10-10 05:12:14--  https://raw.githubusercontent.com/CornerSiow/zero-shot-image-captioning/main/SimilarDict.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1454 (1.4K) [text/plain]\n",
            "Saving to: ‘SimilarDict.py.1’\n",
            "\n",
            "SimilarDict.py.1    100%[===================>]   1.42K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-10-10 05:12:14 (20.3 MB/s) - ‘SimilarDict.py.1’ saved [1454/1454]\n",
            "\n",
            "--2022-10-10 05:12:14--  https://raw.githubusercontent.com/CornerSiow/zero-shot-image-captioning/main/data/categories_places365.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6833 (6.7K) [text/plain]\n",
            "Saving to: ‘categories_places365.txt.2’\n",
            "\n",
            "categories_places36 100%[===================>]   6.67K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-10-10 05:12:14 (51.8 MB/s) - ‘categories_places365.txt.2’ saved [6833/6833]\n",
            "\n",
            "--2022-10-10 05:12:15--  https://raw.githubusercontent.com/CornerSiow/zero-shot-image-captioning/main/data/labels_sunattribute.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 909 [text/plain]\n",
            "Saving to: ‘labels_sunattribute.txt.2’\n",
            "\n",
            "labels_sunattribute 100%[===================>]     909  --.-KB/s    in 0s      \n",
            "\n",
            "2022-10-10 05:12:15 (23.9 MB/s) - ‘labels_sunattribute.txt.2’ saved [909/909]\n",
            "\n",
            "--2022-10-10 05:12:15--  https://raw.githubusercontent.com/CornerSiow/zero-shot-image-captioning/main/Places365.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7412 (7.2K) [text/plain]\n",
            "Saving to: ‘Places365.py.4’\n",
            "\n",
            "Places365.py.4      100%[===================>]   7.24K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-10-10 05:12:15 (22.1 MB/s) - ‘Places365.py.4’ saved [7412/7412]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install 'git+https://github.com/facebookresearch/detectron2.git'\n",
        "!wget https://raw.githubusercontent.com/CornerSiow/zero-shot-image-captioning/main/SimilarDict.py\n",
        "!wget https://raw.githubusercontent.com/CornerSiow/zero-shot-image-captioning/main/data/categories_places365.txt\n",
        "!wget https://raw.githubusercontent.com/CornerSiow/zero-shot-image-captioning/main/data/labels_sunattribute.txt\n",
        "!wget https://raw.githubusercontent.com/CornerSiow/zero-shot-image-captioning/main/Places365.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from SimilarDict import SimilarDict\n",
        "from Places365 import Places365\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.data import MetadataCatalog\n",
        "from nltk.tokenize import word_tokenize, MWETokenizer\n",
        "import os\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "r-NMJDOF9evC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_coco_labels():\n",
        "    things = []\n",
        "    stuff = []\n",
        "    cfg = get_cfg()\n",
        "    cfg.merge_from_file(model_zoo.get_config_file(\"COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml\"))\n",
        "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml\")\n",
        "    meta = MetadataCatalog.get(cfg.DATASETS.TRAIN[0])\n",
        "    for v in meta.thing_classes:\n",
        "        things.append(v.replace(\"-\", \" \").replace(\"_\", \" \"))\n",
        "       \n",
        "    for v in meta.stuff_classes:\n",
        "        stuff.append(v.replace(\"-\", \" \").replace(\"_\", \" \"))\n",
        "       \n",
        "    return things, stuff\n"
      ],
      "metadata": {
        "id": "Z5OGC4OS-NXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_palces365_labels():\n",
        "    # prepare all the labels\n",
        "    # scene category relevant\n",
        "    file_name_category = 'categories_places365.txt'\n",
        "    if not os.access(file_name_category, os.W_OK):\n",
        "        synset_url = 'https://raw.githubusercontent.com/csailvision/places365/master/categories_places365.txt'\n",
        "        os.system('wget ' + synset_url)\n",
        "    classes = list()\n",
        "    with open(file_name_category) as class_file:\n",
        "        for line in class_file:\n",
        "            classes.append(line.strip().split(' ')[0][3:])\n",
        "    classes = list(classes)\n",
        "    for i, v in enumerate(classes):\n",
        "        classes[i] = v.replace(\"_\", \" \").replace(\"/\", \" \")\n",
        "   \n",
        "    # scene attribute relevant\n",
        "    file_name_attribute = 'labels_sunattribute.txt'\n",
        "    if not os.access(file_name_attribute, os.W_OK):\n",
        "        synset_url = 'https://raw.githubusercontent.com/csailvision/places365/master/labels_sunattribute.txt'\n",
        "        os.system('wget ' + synset_url)\n",
        "    with open(file_name_attribute) as f:\n",
        "        lines = f.readlines()\n",
        "        labels_attribute = [item.rstrip() for item in lines]\n",
        "    \n",
        "    for i, v in enumerate(labels_attribute):\n",
        "        labels_attribute[i] = v.replace(\"-\", \" \")\n",
        "\n",
        "    return classes, labels_attribute\n"
      ],
      "metadata": {
        "id": "6bg7-pkJ-TfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similarDict = SimilarDict(0.69)\n",
        "place365 = Places365()\n",
        "places365Class, places365Attribute = load_palces365_labels()\n",
        "things, stuff = load_coco_labels()"
      ],
      "metadata": {
        "id": "ZjlMytiIKTmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "symbolic_list = things + stuff + places365Class + places365Attribute\n",
        "print(\"All available symbolic: {:d}\".format(len(symbolic_list)))\n",
        "print(\"Remove repeated symbolic...\")\n",
        "symbolic_list = list(set(symbolic_list))\n",
        "for i, v in enumerate(symbolic_list):\n",
        "    symbolic_list[i] = v.replace(\" \", \"_\")\n",
        "print(\"All unique symbolic: {:d}\".format(len(symbolic_list)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-o_C3J60_PLE",
        "outputId": "0a9353cd-6e2e-48ad-af0c-d713f5f779ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All available symbolic: 601\n",
            "Remove repeated symbolic...\n",
            "All unique symbolic: 587\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#finding all similar words:\n",
        "print(\"Finding similar symbolic words...\")\n",
        "similar_symbolic = {}\n",
        "similarDict = SimilarDict(threshold=0.69)\n",
        "for symbolic in tqdm(symbolic_list):\n",
        "    # Get the unique symbolic index\n",
        "    symbolic_index = symbolic_list.index(symbolic)\n",
        "    \n",
        "    # Check is unique symbolic is in the similar list?\n",
        "    if symbolic not in similar_symbolic:\n",
        "        similar_symbolic[symbolic] = []\n",
        "    \n",
        "    # append the symbolic index to the similar word\n",
        "    if symbolic_index not in similar_symbolic[symbolic]:\n",
        "        similar_symbolic[symbolic].append(symbolic_index)\n",
        "   \n",
        "    result = similarDict(symbolic.replace(\"_\", \" \"))  \n",
        "    for r in result:\n",
        "        word = r[0]\n",
        "        if word not in similar_symbolic:\n",
        "            similar_symbolic[word] = []\n",
        "            \n",
        "        # append the symbolic index to the similar word\n",
        "        if symbolic_index not in similar_symbolic[word]:\n",
        "            similar_symbolic[word].append(symbolic_index)\n",
        "    \n",
        "\n",
        "\n",
        "print(\"All similar symbolic: {:d}\".format(len(similar_symbolic)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GT3x9M5N_dxc",
        "outputId": "0a84df2c-8fc5-48c1-ab36-c376c622a13d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finding similar symbolic words...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 587/587 [03:45<00:00,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All similar symbolic: 836\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Save Symbolic List...\")\n",
        "with open('symbolic_list.pickle', 'wb') as handle:\n",
        "    pickle.dump(symbolic_list, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    \n",
        "with open('similar_symbolic.pickle', 'wb') as handle:\n",
        "    pickle.dump(similar_symbolic, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96dnEaT3_rT0",
        "outputId": "6c2f376f-9e2c-47ae-9e54-adbd4f11f4f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Save Symbolic List...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Generate Vocabulary"
      ],
      "metadata": {
        "id": "qAZhxZBVX6FY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/CornerSiow/zero-shot-image-captioning/main/Vocabulary.py\n",
        "!wget https://raw.githubusercontent.com/CornerSiow/zero-shot-image-captioning/main/data/caption_label.txt\n",
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-wfDlLtXpBu",
        "outputId": "b0535ab6-2424-4622-a3e8-139239faddb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-10 05:33:46--  https://raw.githubusercontent.com/CornerSiow/zero-shot-image-captioning/main/Vocabulary.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3150 (3.1K) [text/plain]\n",
            "Saving to: ‘Vocabulary.py.3’\n",
            "\n",
            "\rVocabulary.py.3       0%[                    ]       0  --.-KB/s               \rVocabulary.py.3     100%[===================>]   3.08K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-10-10 05:33:46 (38.9 MB/s) - ‘Vocabulary.py.3’ saved [3150/3150]\n",
            "\n",
            "--2022-10-10 05:33:46--  https://raw.githubusercontent.com/CornerSiow/zero-shot-image-captioning/main/data/caption_label.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 192 [text/plain]\n",
            "Saving to: ‘caption_label.txt.2’\n",
            "\n",
            "caption_label.txt.2 100%[===================>]     192  --.-KB/s    in 0s      \n",
            "\n",
            "2022-10-10 05:33:46 (6.41 MB/s) - ‘caption_label.txt.2’ saved [192/192]\n",
            "\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from click->nltk) (5.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->click->nltk) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->click->nltk) (3.8.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from Vocabulary import Vocabulary\n",
        "nltk.download('punkt')\n",
        "vocab = Vocabulary()\n",
        "vocab.loadCaptions(\"caption_label.txt\")\n",
        "vocab.saveFile('vocab.pickle')\n",
        "print(\"Save Vocab...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mqxq_9R6YcLY",
        "outputId": "95a0d860-115e-4491-fed9-6cf59784567a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Save Vocab...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 4: Remove Non-related Symbolics"
      ],
      "metadata": {
        "id": "zKwGAGhAbf1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Unique Symbolic: {:d}\".format(len(symbolic_list)))\n",
        "print(\"Similar Symbolic: {:d}\".format(len(similar_symbolic)))\n",
        "tokenize = MWETokenizer()\n",
        "for v in symbolic_list:    \n",
        "    tokenize.add_mwe(v.split(\"_\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHhB1ThpbZfs",
        "outputId": "a4fc051c-dad5-4eed-fec5-784ddaca3250"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique Symbolic: 587\n",
            "Similar Symbolic: 836\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Read Training Caption...\")\n",
        "print(\"Start filtering unrelated symbolic...\")\n",
        "annFile = \"caption_label.txt\"\n",
        "filtered_symbolic = []\n",
        "with open(annFile) as file:\n",
        "    for line in file:\n",
        "        cap = line.strip().lower()               \n",
        "        tokens = tokenize.tokenize(word_tokenize(cap)) \n",
        "        activated = []\n",
        "        for w in tokens:            \n",
        "            if w in similar_symbolic:\n",
        "                for i in similar_symbolic[w]:\n",
        "                    activated.append(symbolic_list[i])\n",
        "        \n",
        "        for w in activated:\n",
        "          if w not in filtered_symbolic:\n",
        "            filtered_symbolic.append(w)    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0u8uv_cYbrc3",
        "outputId": "183135e6-c6c2-4b3a-e652-31e2414cab2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read Training Caption...\n",
            "Start filtering unrelated symbolic...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Filtered Result...\")\n",
        "print(filtered_symbolic)\n",
        "print(\"Filtered Symbolic: {:d}\".format(len(filtered_symbolic)))\n",
        "\n",
        "with open('filtered_symbolic.pickle', 'wb') as handle:\n",
        "    pickle.dump(filtered_symbolic, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "fJmv-vPDbx3j",
        "outputId": "346d0063-0fe3-49f4-d0f5-1da0d72a73ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Result...\n",
            "['person', 'laptop', 'office', 'banana', 'sink', 'bicycle', 'sky', 'bus']\n",
            "Filtered Symbolic: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 5: Generate Training Data"
      ],
      "metadata": {
        "id": "qpenr-kXc8NF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"Start Generate Training Data...\")\n",
        "annFile = \"caption_label.txt\"\n",
        "data = []\n",
        "with open(annFile) as file:\n",
        "    for line in file:\n",
        "        cap = line.strip().lower()       \n",
        "        feature = np.zeros(len(filtered_symbolic))\n",
        "        tokens = tokenize.tokenize(word_tokenize(cap)) \n",
        "        activated_symbolic = []\n",
        "        for w in tokens:            \n",
        "            if w in similar_symbolic:\n",
        "                for i in similar_symbolic[w]:\n",
        "                    # if symbolic_list[i] not in ['sky','waiting_in_line','motorcycle','biking', 'man_made', 'truck','train','train_station_platform']:\n",
        "                        activated_symbolic.append(symbolic_list[i])\n",
        "           \n",
        "        \n",
        "        \n",
        "        for symbolic in activated_symbolic:\n",
        "            if symbolic in filtered_symbolic:               \n",
        "                feature[filtered_symbolic.index(symbolic)] = 1\n",
        "       \n",
        "        # feature = feature / np.max(feature)\n",
        "        \n",
        "        print('----------------------------------')\n",
        "        print(cap, '\\n',activated_symbolic,'\\n', feature)        \n",
        "                \n",
        "        data.append((torch.from_numpy(feature), vocab.convertSentenceToToken(cap)))\n",
        "print('----------------------------------')\n",
        "print(\"Finish Generate Training Data...\")        \n",
        "print(\"Save Training Data...\")\n",
        "with open('training_data.pickle', 'wb') as handle:\n",
        "    pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYwvTdchdAq_",
        "outputId": "f1368eab-26db-4628-d4cd-2ffd3ade68e4"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Generate Training Data...\n",
            "----------------------------------\n",
            "a person using a laptop in the office \n",
            " ['person', 'laptop', 'office'] \n",
            " [1. 1. 1. 0. 0. 0. 0. 0.]\n",
            "----------------------------------\n",
            "a person eats a banana in front of a laptop \n",
            " ['person', 'banana', 'laptop'] \n",
            " [1. 1. 0. 1. 0. 0. 0. 0.]\n",
            "----------------------------------\n",
            "a person washes his face in the sink \n",
            " ['person', 'sink'] \n",
            " [1. 0. 0. 0. 1. 0. 0. 0.]\n",
            "----------------------------------\n",
            "a person riding a bike on a clear sky \n",
            " ['person', 'bicycle', 'sky'] \n",
            " [1. 0. 0. 0. 0. 1. 1. 0.]\n",
            "----------------------------------\n",
            "someone is waiting at the bus stop \n",
            " ['person', 'bus'] \n",
            " [1. 0. 0. 0. 0. 0. 0. 1.]\n",
            "----------------------------------\n",
            "Finish Generate Training Data...\n",
            "Save Training Data...\n"
          ]
        }
      ]
    }
  ]
}